Traditionally, strings are conceptualized as sequences of characters, which mathematically is a function from indices to characters. When all characters are exactly one byte, this can be conveniently and efficiently represented with a plain old array of bytes. It's simple and efficient. End of story.

Unicode complicates things because common encodings like UTF-8 and UTF-16 are variable-width, meaning that characters require different numbers of bytes in their representation. In UTF-8, for example, all of the ASCII characters are one byte, exactly identical to their traditional ASCII encoding; all other characters, however, take between two and four bytes to represent. This means that you can't efficiently look up the kth character of a string by doing array indexing anymore: the kth byte of the representation is no longer the kth character. Worse still, some bytes are not even the beginning of a character at all — they're right in the middle of a character, i.e. what is known as a continuation character. Fortunately, however, UTF-8 is carefully designed so that you can immediately tell which.

If you allow people direct access to the bytes of an encoded string, they can and will do all sorts of bad things that produce invalid unicode and broken behaviors when unicode input is received. If you provide a string abstraction that makes strings behave like functions from indices to unicode characters, then you have to chose one of the following:

  1. Have fast, natural random access to characters, but store strings so that every character is 32 bits wide (the natural choice for this is UTF-32). This also requires processing every string before accessing or manipulating it to convert it from its input encoding (usually UTF-8) to UTF-32. Obviously, this introduces a lot of overhead.

  2. Keep strings in their input form, but make indexing into a string an O(n) operation which requires scanning the entire string from the beginning to find the kth character from there. This turns a lot of trivial O(n) string processing algorithms into O(n^2) algorithms unless you completely avoid indexing into strings altogether.

  3. Represent strings by some clever, relatively complicated data structure that allows strings to be input in their raw encoded form, but somehow makes indexing the kth character look like it's faster then O(n) at least amortized over a lot of operations. A relatively easy way to do this is to remember the last few indices that were accessed and figure out new indices relative to them.

  4. Don't allow indexing into strings. Instead, only provide mechanisms — like iterating over the characters — that are unicode safe and can be implemented efficiently.

  5. Ignore the whole issue and just equate strings and with byte arrays and let unicode go fuck itself (aka the C approach).

Technically, #4 doesn't really keep the abstraction of strings as functions from indices to characters, so maybe it doesn't belong here, and #5 isn't a solution at all, but rather a matter of deciding that you don't care.

Our idea is to change the abstraction of what a string is instead of choosing any of the above. Rather than conceptualizing a string as a function from indices to characters, we're proposing that a string be a _partial_ function from indices to characters. What does this mean? It means that for some indices no character value is returned — i.e. instead of returning a value, the language throws an exception. So in short str[k] will throw an exception if k doesn't correspond to a character. For UTF-8 strings that are plain ASCII this will never happen. For UTF-32 strings it will never happen. For UTF-16 strings that only include the common two-byte characters it will never happen. For other encoded strings, it could happen though.

Why does this modified abstraction help? It recognizes that bytes and characters are not one-to-one in many encodings, but still allows you store strings in their native encoding and retrieve characters efficiently by their byte index. If you try to get or set a character at an index that isn't the start of a character, it's an exception, indicating that the partial function isn't defined there. The "string" as one normally thinks of it, is the sequence characters returned by the string as the indices range from 1 to the maximum index. The gotcha is that some indices don't return a character.

Code that naively just iterates through all indices will work and work at C-like speed until it encounters strange input data, at which point it will throw a nice clean exception letting you know exactly what the problem is. You can probably just modify the code to catch the exception and move on to the next index, or you can rewrite it to use a unicode-safe character iterator instead. Also, if you just consider string indices as opaque objects and don't try to do arithmetic with them (iterating through them counts as arithmetic: k += 1), then as long as you only ever index into a string with an index given to you by some string function, this kind of thing will never happen.

In short, by changing the string abstraction, we can support the full range of string encodings at C efficiency and with a nice, safe, clean programmatic interface.
